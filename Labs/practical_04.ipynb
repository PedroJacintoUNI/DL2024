{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron and Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 \n",
    "\n",
    "Consider the neural network considered in the first question of the theoretical component of the practical class, with number of units: 4,4,3,3.\n",
    "\n",
    "![](https://drive.google.com/uc?id=1SHUgdosKp6AX8rRAACCZ5nb4kUXreI3g)\n",
    "\n",
    "Assume all units, except the ones in the output layer, use the hyperbolic tangent activation function. \n",
    "\n",
    "Consider the following training example:\n",
    "\n",
    "$\\mathbf{x} =\\begin{bmatrix} 1, 0, 1, 0 \\end{bmatrix}^\\intercal $,   $\\mathbf{y} =\\begin{bmatrix} 0\\\\ 1\\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "❓ Using the squared error loss do a stochastic gradient descent update, initializing all connection weights and biases to 0.1 and a  learning rate η = 0.1:\n",
    "\n",
    "1. Perform the forward pass\n",
    "2. Compute the loss\n",
    "3. Compute gradients with backpropagation\n",
    "4. Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[1, 0, 1, 0]])\n",
    "labels = np.array([[0, 1, 0]])\n",
    "\n",
    "# First is input size, last is output size.\n",
    "units = [4, 4, 3, 3]\n",
    "\n",
    "# Initialize weights with correct shapes \n",
    "W1 = 0.1 * np.ones((units[1], units[0]))\n",
    "b1 = 0.1 * np.ones(units[1])\n",
    "\n",
    "W2 = 0.1 * np.ones((units[2], units[1]))\n",
    "b2 = 0.1 * np.ones(units[2])\n",
    "\n",
    "W3 = 0.1 * np.ones((units[3], units[2]))\n",
    "b3 = 0.1 * np.ones(units[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29131261 0.29131261 0.29131261 0.29131261]\n",
      "[0.16396106 0.16396106 0.16396106]\n",
      "[0.21320353 0.21320353 0.21320353]\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass\n",
    "h0 = inputs[0]\n",
    "\n",
    "z1 = W1.dot(h0) + b1\n",
    "h1 = np.tanh(z1)\n",
    "print(h1)\n",
    "\n",
    "z2 = W2.dot(h1) + b2\n",
    "h2 = np.tanh(z2)\n",
    "\n",
    "z3 = W3.dot(h2) + b3\n",
    "print(z3)\n",
    "print(h2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37636378397755565\n"
     ]
    }
   ],
   "source": [
    "# Loss\n",
    "\n",
    "y = labels[0]\n",
    "\n",
    "loss = 0.5*(z3 - y).dot(z3 - y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03495708  0.03495708  0.03495708]\n",
      " [-0.17824646 -0.17824646 -0.17824646]\n",
      " [ 0.03495708  0.03495708  0.03495708]]\n",
      "[[-0.01278356 -0.01278356 -0.01278356 -0.01278356]\n",
      " [-0.01278356 -0.01278356 -0.01278356 -0.01278356]\n",
      " [-0.01278356 -0.01278356 -0.01278356 -0.01278356]]\n",
      "[[-0.00869597  0.         -0.00869597  0.        ]\n",
      " [-0.00869597  0.         -0.00869597  0.        ]\n",
      " [-0.00869597  0.         -0.00869597  0.        ]\n",
      " [-0.00869597  0.         -0.00869597  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Backpropagation\n",
    "grad_z3 = z3 - y\n",
    "#print(grad_z3)\n",
    "\n",
    "#gradient of hidden parameters\n",
    "grad_W3 = grad_z3[:, None].dot(h2[:, None].T)\n",
    "#print(h2)\n",
    "grad_b3 = grad_z3\n",
    "print(grad_W3)\n",
    "#print(grad_b3)\n",
    "\n",
    "#gradient of hidden layers below\n",
    "grad_h2 = W3.T.dot(grad_z3)\n",
    "#print(grad_h2)\n",
    "\n",
    "#gradient of hidden layer before activation\n",
    "grad_z2 = grad_h2.dot(1-(h2.dot(h2)))\n",
    "#print(grad_z2)\n",
    "\n",
    "#gradient of hidden parameters\n",
    "grad_W2 = grad_z2[:, None].dot(h1[:, None].T)\n",
    "#print(h1)\n",
    "grad_b2 = grad_z2\n",
    "print(grad_W2)\n",
    "#print(grad_b2)\n",
    "\n",
    "#gradient of hidden layer below\n",
    "grad_h1 = W2.T.dot(grad_z2)\n",
    "#print(grad_h1)\n",
    "\n",
    "# Gradient of hidden layer below before activation.\n",
    "grad_z1 = grad_h1.dot(1-h1.dot(h1))\n",
    "#print(grad_z1)\n",
    "\n",
    "# Gradient of hidden parameters.\n",
    "grad_W1 = grad_z1[:, None].dot(h0[:, None].T)\n",
    "grad_b1 = grad_z1\n",
    "print(grad_W1)\n",
    "#print(grad_b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Gradients\n",
    "eta = 0.1\n",
    "# Gradient updates.\n",
    "W1 -= eta * grad_W1\n",
    "b1 -= eta * grad_b1\n",
    "print(W1)\n",
    "print(b1)\n",
    "\n",
    "W2 -= eta * grad_W2\n",
    "b2 -= eta * grad_b2\n",
    "print(W2)\n",
    "print(b2)\n",
    "\n",
    "W3 -= eta * grad_W3\n",
    "b3 -= eta * grad_b3\n",
    "print(W3)\n",
    "print(b3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Let's say we were using the same training example but with the following changes:\n",
    "- The output units have a softmax activation function\n",
    "- The error function is cross-entropy\n",
    "\n",
    "Keeping the same initializations and learning rate, adjust your computations to the new changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** We need only to change:  \n",
    "- the output, *i.e.*, $\\hat{y} = softmax(z_3)$ instead of $\\hat{y} = z_3$\n",
    "- the loss computation to $L = -y.log(\\hat{y})$\n",
    "- the gradient of the loss with respect to $z_3$: $\\frac{dL}{dz_3}$\n",
    "\n",
    "All other steps remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete functions `forward`, `compute_loss`, `backpropagation` and `update_weights` generalized to perform the same computations as before, but for any MLP architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "x: single observation of shape (n,)\n",
    "weights: list of weight matrices [W1, W2, ...]\n",
    "biases: list of biases matrices [b1, b2, ...]\n",
    "\n",
    "y: final output\n",
    "hiddens: list of computed hidden layers [h1, h2, ...]\n",
    "'''\n",
    "def Relu(x):\n",
    "    return (np.maximum(0,x))\n",
    "\n",
    "def forward(x, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    #hidden layers\n",
    "    hiddens = []\n",
    "    #z values from start to finish\n",
    "    output = []\n",
    "        \n",
    "    for i in range(num_layers):\n",
    "        h = x if i == 0 else hiddens[i -1]\n",
    "        z = weights[i].dot(h) + biases[i]\n",
    "        if i < num_layers -1:\n",
    "           hiddens.append(Relu(z))\n",
    "        if i == num_layers-1:\n",
    "            z -= np.max(z)\n",
    "        output.append(z)\n",
    "    return output, hiddens\n",
    "\n",
    "def compute_loss(output, y):\n",
    "    # compute loss\n",
    "    #softmax\n",
    "    probs = np.exp(output) / np.sum(np.exp(output))\n",
    "    loss = -y.dot(np.log(probs))\n",
    "    \n",
    "    return loss   \n",
    "\n",
    "def backward(x, y, output, hiddens, weights):\n",
    "    num_layers = len(self.weights)\n",
    "        \n",
    "        #one hot enconding\n",
    "        output = np.zeros(self.weights[num_layers -1].shape[0])\n",
    "        output[y] = 1\n",
    "        \n",
    "        #softmax calculation\n",
    "        probs = np.exp(z_values[num_layers-1]) / np.sum(np.exp(z_values[num_layers-1]))\n",
    "        #gradient calculation\n",
    "        grad_z = probs - output\n",
    "        \n",
    "        grad_weights = []\n",
    "        grad_bias = []\n",
    "        for i in range(num_layers-1, -1, -1):\n",
    "            # Gradient of hidden parameters.\n",
    "            h = x if i == 0 else hiddens[i-1]\n",
    "        \n",
    "            grad_weights.append(grad_z[:, None].dot(h[:, None].T))\n",
    "            grad_bias.append(grad_z)\n",
    "\n",
    "            # Gradient of hidden layer below.\n",
    "            grad_h = self.weights[i].T.dot(grad_z)\n",
    "            \n",
    "            if i >= 1:\n",
    "                mask = z_values[i-1]>0\n",
    "                grad_z = grad_h*mask\n",
    "             \n",
    "        grad_weights.reverse()\n",
    "        grad_bias.reverse()\n",
    "        return grad_weights, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Now we will use the MLP on real data to classify handwritten digits.\n",
    "\n",
    "Data is loaded, split into train and test sets and target is one-hot encoded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_digits()\n",
    "\n",
    "inputs = data.data  \n",
    "labels = data.target  \n",
    "n, p = np.shape(inputs)\n",
    "n_classes = len(np.unique(labels))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode labels as one-hot vectors.\n",
    "one_hot = np.zeros((np.size(y_train, 0), n_classes))\n",
    "for i in range(np.size(y_train, 0)):\n",
    "    one_hot[i, y_train[i]] = 1\n",
    "y_train_ohe = one_hot\n",
    "one_hot = np.zeros((np.size(y_test, 0), n_classes))\n",
    "for i in range(np.size(y_test, 0)):\n",
    "    one_hot[i, y_test[i]] = 1\n",
    "y_test_ohe = one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_train_epoch` using your previously defined functions to compute one epoch of training using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Outputs:\n",
    "    - weights: list of updated weights\n",
    "    - biases: list of updated \n",
    "    - loss: scalar of total loss (sum for all observations)\n",
    "\n",
    "'''\n",
    "\n",
    "def MLP_train_epoch(inputs, labels, weights, biases):\n",
    "    num_layers = len(weights)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # For each observation and target\n",
    "    for x, y in zip(inputs, labels):\n",
    "        # Compute forward pass\n",
    "        output, hiddens = forward(x, weights, biases)\n",
    "        \n",
    "        # Compute Loss and update total loss\n",
    "        loss = compute_loss(output, y)\n",
    "        total_loss += loss\n",
    "        # Compute backpropagation\n",
    "        grad_weights, grad_biases = backward(x, y, output, hiddens, weights)      \n",
    "        # Update weights\n",
    "        num_layers = len(weights)\n",
    "        for i in range(num_layers):\n",
    "            weights[i] -= eta*grad_weights[i]\n",
    "            biases[i] -= eta*grad_biases[i]\n",
    "            \n",
    "    return weights, biases, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a MLP with a single hidden layer of 50 units and a learning rate of $0.001$. \n",
    "\n",
    "❓ Run 100 epochs of your MLP. Save the loss at each epoch in a list and plot the loss evolution after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3329.0891945355424\n",
      "3330.471368157632\n",
      "3330.473199053035\n",
      "3330.473200480191\n",
      "3330.473200481368\n",
      "3330.473200481369\n",
      "3330.473200481369\n",
      "3330.4732004813695\n",
      "3330.473200481369\n",
      "3330.4732004813686\n",
      "3330.473200481369\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n",
      "3330.4732004813686\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Utilizador\\Desktop\\1AnoMestrado\\AProf\\Labs\\practical_04.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Run epochs and append loss to list\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     weights, biases, current_loss \u001b[39m=\u001b[39m MLP_train_epoch(X_train, y_train_ohe, weights, biases)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     loss\u001b[39m.\u001b[39mappend(current_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mprint\u001b[39m(current_loss)\n",
      "\u001b[1;32mc:\\Users\\Utilizador\\Desktop\\1AnoMestrado\\AProf\\Labs\\practical_04.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Compute backpropagation\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m grad_weights, grad_biases \u001b[39m=\u001b[39m backward(x, y, output, hiddens, weights)      \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Update weights\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m num_layers \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(weights)\n",
      "\u001b[1;32mc:\\Users\\Utilizador\\Desktop\\1AnoMestrado\\AProf\\Labs\\practical_04.ipynb Cell 19\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_layers\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m# Gradient of hidden parameters.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     h \u001b[39m=\u001b[39m x \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m hiddens[i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     grad_weights\u001b[39m.\u001b[39mappend(grad_z[:, \u001b[39mNone\u001b[39;49;00m]\u001b[39m.\u001b[39;49mdot(h[:, \u001b[39mNone\u001b[39;49;00m]\u001b[39m.\u001b[39;49mT))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     grad_biases\u001b[39m.\u001b[39mappend(grad_z)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Utilizador/Desktop/1AnoMestrado/AProf/Labs/practical_04.ipynb#X24sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     \u001b[39m# Gradient of hidden layer below.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "units =[64, 50, 10]\n",
    "\n",
    "# Initialize weights\n",
    "W1 = 0.1 * np.ones((units[1], units[0]))\n",
    "b1 = 0.1 * np.ones(units[1])\n",
    "\n",
    "W2 = 0.1 * np.ones((units[2], units[1]))\n",
    "b2 = 0.1 * np.ones(units[2])\n",
    "\n",
    "\n",
    "weights = [W1, W2]\n",
    "biases = [b1, b2]\n",
    "\n",
    "# Empty loss list\n",
    "loss = []\n",
    "# Learning rate.\n",
    "eta = 0.001\n",
    "    \n",
    "# Run epochs and append loss to list\n",
    "for epoch in range(50):\n",
    "    weights, biases, current_loss = MLP_train_epoch(X_train, y_train_ohe, weights, biases)\n",
    "    loss.append(current_loss)\n",
    "    print\n",
    "# Plot loss evolution\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Evolution Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Complete function `MLP_predict` to get array of predictions from your trained MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_predict(inputs, weights, biases):\n",
    "    predicted_labels = []\n",
    "    for x in inputs:\n",
    "        # Compute forward pass and get the class with the highest probability\n",
    "        output, _ = forward(x, weights, biases)\n",
    "        y_hat = np.zeros_like(output)\n",
    "        y_hat[np.argmax(output)] = 1\n",
    "        predicted_labels.append(y_hat)\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Compute the accuracy on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.72%\n",
      "Accuracy: 7.78%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predict1 = MLP_predict(X_train, weights, biases)\n",
    "\n",
    "predicted1 = np.argmax(predict1, axis=1)\n",
    "\n",
    "\n",
    "accuracy1 = accuracy_score(np.argmax(y_train_ohe, axis=1), predicted1)\n",
    "print(f'Accuracy: {accuracy1 * 100:.2f}%')\n",
    "\n",
    "predict = MLP_predict(X_test, weights, biases)\n",
    "\n",
    "\n",
    "predicted = np.argmax(predict, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(np.argmax(y_test_ohe, axis=1), predicted)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our results with Sklearn's implementation of the MLP. Compare their accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50),\n",
    "                    activation='tanh',\n",
    "                    solver='sgd',\n",
    "                    learning_rate='constant',\n",
    "                    learning_rate_init=0.001,\n",
    "                    nesterovs_momentum=False,\n",
    "                    random_state=1,\n",
    "                    max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
